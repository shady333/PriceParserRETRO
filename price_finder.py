import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import time
import re
from concurrent.futures import ThreadPoolExecutor
import threading
import random
import os

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è
BASE_URL = "https://retromagaz.com/hot-wheels?page="
BUY_OUTPUT_FILE = "car_prices.csv"
SELL_OUTPUT_FILE = "sell_car_prices.csv"
PROGRESS_FILE = "progress.txt"
ERROR_LOG_FILE = "scraper_errors.log"
CURRENT_DATE = datetime.now().strftime('%Y-%m-%d')
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/91.0.4472.124",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.5"
}
PRICE_THRESHOLDS = {
    'premium': 350,
    'rlc': 1200,
    'super_treasure_hunt': 900,
    'diorama': 600,
    'matchbox': 90,
    'treasure_hunts': 90,
    'team_transport': 550,
    'mainline': 90
}
WORDS_TO_IGNORE = {
    "–ù–∞–±—ñ—Ä"
}
PAGES_PER_DAY = 60
SAVE_INTERVAL = 5
MAX_WORKERS = 20

# –ü–æ—Ç–æ–∫–æ–±–µ–∑–ø–µ—á–Ω–∏–π —Å–ø–∏—Å–æ–∫ –¥–ª—è –¥–∞–Ω–∏—Ö
BUY_DATA = []
SELL_DATA = []
DATA_LOCK = threading.Lock()

# –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ —Ñ—ñ–ª—å—Ç—Ä—É–≤–∞–Ω–Ω—è
SKIP_PREMIUM = False
SKIP_RLC = False
SKIP_SUPER_TREASURE_HUNT = False
SKIP_DIORAMA = False
SKIP_MATCHBOX = False
SKIP_TREASURE_HUNTS = False
SKIP_TEAM_TRANSPORT = False


# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –ª–æ–≥—É–≤–∞–Ω–Ω—è –ø–æ–º–∏–ª–æ–∫
def log_error(message):
    with open(ERROR_LOG_FILE, 'a', encoding='utf-8') as f:
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        f.write(f"[{timestamp}] {message}\n")


def extract_sku(car_name):
    """
    –í–∏—Ç—è–≥—É—î SKU –∑ –Ω–∞–∑–≤–∏ —Ç–æ–≤–∞—Ä—É Hot Wheels.
    - –ü–∞—Ç–µ—Ä–Ω: 1‚Äì4 –≤–µ–ª–∏–∫—ñ –ª—ñ—Ç–µ—Ä–∏ + 2‚Äì4 —Ü–∏—Ñ—Ä–∏ (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥: GRN86, T9679, X1666, HYY72)
    - –Ø–∫—â–æ –ø–æ–¥–≤—ñ–π–Ω–∏–π –∫–æ–¥ —á–µ—Ä–µ–∑ '/', –ø–æ–≤–µ—Ä—Ç–∞—î —Ç–æ–π, —â–æ –ø—ñ—Å–ª—è '/'.
    - –Ü–≥–Ω–æ—Ä—É—î –≤–º—ñ—Å—Ç —É –∫—Ä—É–≥–ª–∏—Ö –¥—É–∂–∫–∞—Ö (—â–æ–± –Ω–µ –±—Ä–∞—Ç–∏ –≤–Ω—É—Ç—Ä—ñ—à–Ω—ñ –∫–æ–¥–∏ —Ç–∏–ø—É BNR32).
    """
    if car_name is None:
        return None

    # –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ —Ä—è–¥–æ–∫ –¥–ª—è –ø–æ—à—É–∫—É (–≤–µ–ª–∏–∫—ñ –ª—ñ—Ç–µ—Ä–∏)
    s = str(car_name).upper().strip().strip('"')

    # 1) –ü–æ–¥–≤—ñ–π–Ω–∏–π –∫–æ–¥ —á–µ—Ä–µ–∑ '/' - —à—É–∫–∞—î–º–æ –≤ —É—Å—å–æ–º—É —Ä—è–¥–∫—É —ñ –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ –ø—Ä–∞–≤—É —á–∞—Å—Ç–∏–Ω—É
    double_re = re.search(r'\b[A-Z]{1,4}\d{2,4}/([A-Z]{1,4}\d{2,4})\b', s)
    if double_re:
        return double_re.group(1)

    # 2) –í–∏–¥–∞–ª—è—î–º–æ –≤–º—ñ—Å—Ç —É –¥—É–∂–∫–∞—Ö (—â–æ–± —ñ–≥–Ω–æ—Ä—É–≤–∞—Ç–∏ –º–æ–¥–µ–ª—ñ —Ç–∏–ø—É (BNR32), (R35) —ñ —Ç.–¥.)
    s_no_paren = re.sub(r'\([^)]*\)', ' ', s)

    # 3) –ó–Ω–∞—Ö–æ–¥–∏–º–æ –≤—Å—ñ –ø–æ—Ç–µ–Ω—Ü—ñ–π–Ω—ñ –∫–æ–¥–∏ —ñ –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ–π (–Ω–∞–π—á–∞—Å—Ç—ñ—à–µ SKU —Å—Ç–æ—ó—Ç—å –±–ª–∏–∂—á–µ –¥–æ –∫—ñ–Ω—Ü—è)
    all_codes = re.findall(r'\b[A-Z]{1,4}\d{2,4}\b', s_no_paren)
    if all_codes:
        return all_codes[-1]

    return None


# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Ç–∏–ø—É —Ç–æ–≤–∞—Ä—É, –ø–æ—Ä–æ–≥—É —Ç–∞ –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó
def get_category_and_threshold(title):
    title_lower = title.lower()
    if 'team transport' in title_lower:
        return 'Team Transport', PRICE_THRESHOLDS['team_transport']
    if ('4—à—Ç' in title_lower or '2—à—Ç' in title_lower) and 'diorama' in title_lower:
        return 'Diorama', PRICE_THRESHOLDS['diorama']
    if 'premium' in title_lower:
        return 'Premium', PRICE_THRESHOLDS['premium']
    elif 'rlc' in title_lower:
        return 'RLC', PRICE_THRESHOLDS['rlc']
    elif 'super treasure hunt' in title_lower:
        return 'Super Treasure Hunt', PRICE_THRESHOLDS['super_treasure_hunt']
    elif 'matchbox' in title_lower:
        return 'Matchbox', PRICE_THRESHOLDS['matchbox']
    elif 'treasure hunt' in title_lower:
        return 'Treasure Hunts', PRICE_THRESHOLDS['treasure_hunts']
    return 'MainLine', PRICE_THRESHOLDS['mainline']


# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –æ—á–∏—â–µ–Ω–Ω—è –Ω–∞–∑–≤–∏
def clean_title(title):
    patterns = [
        r'^\s*–ú–∞—à–∏–Ω–∫–∞\s+–ë–∞–∑–æ–≤–∞\s+',
        r'^\s*–¢–µ–º–∞—Ç–∏—á–Ω–∞\s+–ú–∞—à–∏–Ω–∫–∞\s+',
        r'^\s*–ú–∞—à–∏–Ω–∫–∞\s+',
        r'^\s*Hot\s+Wheels\s+',
        r'^\s*Premium\s+Hot\s+Wheels\s+',
        r'^\s*Matchbox\s+'
    ]
    clean = title
    for pattern in patterns:
        clean = re.sub(pattern, '', clean, flags=re.IGNORECASE)
    return clean.strip()


def check_ignore_words(text):
    return any(word in text for word in WORDS_TO_IGNORE)


# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É —Å—Ç–æ—Ä—ñ–Ω–∫–∏ —Ç–æ–≤–∞—Ä—É
def scrape_product_page(url):
    try:
        time.sleep(random.uniform(1, 3))
        response = requests.get(url, headers=HEADERS, timeout=10)
        if response.status_code != 200:
            print(f"–ü–æ–º–∏–ª–∫–∞: –Ω–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ —Å—Ç–æ—Ä—ñ–Ω–∫—É —Ç–æ–≤–∞—Ä—É {url} (–∫–æ–¥: {response.status_code})")
            return None

        soup = BeautifulSoup(response.text, 'html.parser')
        title_elem = soup.find('div', class_=re.compile('product_title--top'))
        if not title_elem:
            print(f"–ü–æ–º–∏–ª–∫–∞: –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ div.product_title--top –Ω–∞ {url}")
            return None

        title_h1 = title_elem.find('h1') or title_elem.find('p', class_='h1')
        if not title_h1:
            print(f"–ü–æ–º–∏–ª–∫–∞: –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ h1 –∞–±–æ p.h1 —É product_title--top –Ω–∞ {url}")
            return None

        title = title_h1.text.strip()
        if not title or not ('hot wheels' in title.lower() or 'matchbox' in title.lower()):
            print(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ: {title} - –Ω–µ –º–∞—à–∏–Ω–∫–∞")
            return None

        if check_ignore_words(title):
            print(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ: {title} - –º—ñ—Å—Ç–∏—Ç—å —Å–ª–æ–≤–æ –∑ —Å–ø–∏—Å–∫—É –¥–ª—è —ñ–≥–Ω–æ—Ä—É–≤–∞–Ω–Ω—è")
            return None

        # –í–∏—Ç—è–≥—É—î–º–æ SKU
        sku = extract_sku(title)
        if not sku:
            error_msg = f"–ù–µ–º–∞—î SKU: {title} | URL: {url}"
            print(f"‚ö†Ô∏è {error_msg}")
            log_error(error_msg)
            return None

        category, threshold = get_category_and_threshold(title)
        title_lower = title.lower()
        if (SKIP_PREMIUM and 'premium' in title_lower) or \
                (SKIP_RLC and 'rlc' in title_lower) or \
                (SKIP_SUPER_TREASURE_HUNT and 'super treasure hunt' in title_lower) or \
                (SKIP_DIORAMA and 'diorama' in title_lower) or \
                (SKIP_MATCHBOX and 'matchbox' in title_lower) or \
                (SKIP_TREASURE_HUNTS and 'treasure hunt' in title_lower) or \
                (SKIP_TEAM_TRANSPORT and 'team transport' in title_lower):
            print(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ: {title} - —Ñ—ñ–ª—å—Ç—Ä –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó")
            return None

        # Buying price
        price_elem = soup.find('div', class_='product_info--shoping-bar')
        if not price_elem or not price_elem.find('span', class_='price'):
            print(f"–ü–æ–º–∏–ª–∫–∞: –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ —Ü—ñ–Ω—É –ø–æ–∫—É–ø–∫–∏ –Ω–∞ {url}")
            return None
        price_text = price_elem.find('span', class_='price').text.strip()
        price_text = re.sub(r'[^\d.]', '', price_text)
        buy_price = float(price_text)

        # Selling price
        sell_price_elem = soup.find('p', class_='product_options-price')
        if not sell_price_elem:
            print(f"–ü–æ–º–∏–ª–∫–∞: –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ —Ü—ñ–Ω—É –ø—Ä–æ–¥–∞–∂—É –Ω–∞ {url}")
            return None

        promo_price_elem = sell_price_elem.find('span', class_='red-text')
        if promo_price_elem:
            sell_price_text = promo_price_elem.text.strip()
        else:
            sell_price_text = sell_price_elem.text.strip()

        sell_price_text = re.sub(r'[^\d.]', '', sell_price_text)
        sell_price = float(sell_price_text)

        if buy_price >= threshold:
            print(f"‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ: SKU={sku} | {title} | –ö—É–ø—ñ–≤–ª—è={buy_price}, –ü—Ä–æ–¥–∞–∂={sell_price} (–ø–æ—Ä—ñ–≥ {threshold})")
            clean_name = clean_title(title)
            if not clean_name:
                print(f"–ü–æ–º–∏–ª–∫–∞: –æ—á–∏—â–µ–Ω–∞ –Ω–∞–∑–≤–∞ –ø–æ—Ä–æ–∂–Ω—è –¥–ª—è {title} –Ω–∞ {url}")
                return None

            # –í–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è URL –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
            image_url = None
            product_image = soup.find('div', class_='product_image')
            if product_image:
                picture = product_image.find('picture')
                if picture:
                    source = picture.find('source')
                    if source and 'srcset' in source.attrs:
                        image_url = 'https://retromagaz.com' + source['srcset'].split()[0]
                    elif picture.find('img') and 'src' in picture.find('img').attrs:
                        image_url = 'https://retromagaz.com' + picture.find('img')['src']

            return {
                'sku': sku,
                'car_name': clean_name,
                'buy_price': buy_price,
                'sell_price': sell_price,
                'category': category,
                'image_url': image_url
            }
        else:
            print(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ: {title} - —Ü—ñ–Ω–∞ –ø–æ–∫—É–ø–∫–∏ {buy_price} –Ω–∏–∂—á–µ –ø–æ—Ä–æ–≥—É {threshold}")
            return None

    except Exception as e:
        error_msg = f"–ü–æ–º–∏–ª–∫–∞ –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ {url}: {e}"
        print(error_msg)
        log_error(error_msg)
        return None


# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –æ–Ω–æ–≤–ª–µ–Ω–Ω—è CSV
def update_csv(file_path, data_list, price_key):
    os.makedirs(os.path.dirname(file_path) or '.', exist_ok=True)
    try:
        df = pd.read_csv(file_path, encoding='utf-8-sig')
    except FileNotFoundError:
        df = pd.DataFrame(columns=['sku', 'category', 'car_name', 'image_url'])

    if CURRENT_DATE not in df.columns:
        df[CURRENT_DATE] = pd.NA

    for item in data_list:
        sku = item['sku']
        car_name = item['car_name']
        price = item[price_key]
        category = item['category']
        image_url = item.get('image_url', '')

        car_name = f'"{car_name}"' if ',' in car_name else car_name

        if sku not in df['sku'].values:
            # –ù–æ–≤–∏–π —Ç–æ–≤–∞—Ä
            df.loc[len(df)] = [sku, category, car_name, image_url] + [pd.NA] * (len(df.columns) - 4)
            df.loc[df['sku'] == sku, CURRENT_DATE] = price
        else:
            # –û–Ω–æ–≤–ª—é—î–º–æ —ñ—Å–Ω—É—é—á–∏–π —Ç–æ–≤–∞—Ä
            df.loc[df['sku'] == sku, CURRENT_DATE] = price
            df.loc[df['sku'] == sku, 'category'] = category
            df.loc[df['sku'] == sku, 'car_name'] = car_name  # –û–Ω–æ–≤–ª—é—î–º–æ –Ω–∞–∑–≤—É (–Ω–∞–π–Ω–æ–≤—ñ—à–∞)
            if image_url:
                df.loc[df['sku'] == sku, 'image_url'] = image_url

    columns = ['sku', 'category', 'car_name', 'image_url'] + [col for col in df.columns if
                                                              col not in ['sku', 'category', 'car_name', 'image_url']]
    df = df[columns]
    df.to_csv(file_path, index=False, encoding='utf-8-sig', sep=',')
    print(f"üíæ –î–∞–Ω—ñ –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤ {file_path}")


# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –ø–∞–≥—ñ–Ω–∞—Ü—ñ—ó
def scrape_page(page_num):
    global BUY_DATA, SELL_DATA
    url = f"{BASE_URL}{page_num}"
    print(f"üìÑ –ü–∞—Ä—Å–∏–º–æ —Å—Ç–æ—Ä—ñ–Ω–∫—É {page_num}...")

    try:
        response = requests.get(url, headers=HEADERS)
        if response.status_code != 200:
            print(f"–ü–æ–º–∏–ª–∫–∞: –Ω–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ —Å—Ç–æ—Ä—ñ–Ω–∫—É {url} (–∫–æ–¥: {response.status_code})")
            return False

        soup = BeautifulSoup(response.text, 'html.parser')
        items = soup.find_all('div', class_='game-card')
        if not items:
            print(f"–ü–æ–ø–µ—Ä–µ–¥–∂–µ–Ω–Ω—è: –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä—ñ–≤ –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ {page_num}")
            return False

        product_urls = [item.find('a', class_='game-card__image')['href'] for item in items if
                        item.find('a', class_='game-card__image') and item.find('a', class_='game-card__image').get(
                            'href')]

        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            futures = [executor.submit(scrape_product_page, url) for url in product_urls]
            for future in futures:
                result = future.result()
                if result:
                    with DATA_LOCK:
                        BUY_DATA.append({
                            'sku': result['sku'],
                            'car_name': result['car_name'],
                            'price': result['buy_price'],
                            'category': result['category'],
                            'image_url': result['image_url']
                        })
                        SELL_DATA.append({
                            'sku': result['sku'],
                            'car_name': result['car_name'],
                            'price': result['sell_price'],
                            'category': result['category'],
                            'image_url': result['image_url']
                        })

        return True

    except Exception as e:
        error_msg = f"–ü–æ–º–∏–ª–∫–∞ –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ {url}: {e}"
        print(error_msg)
        log_error(error_msg)
        return False


# –ì–æ–ª–æ–≤–Ω–∞ –ª–æ–≥—ñ–∫–∞
def main():
    global BUY_DATA, SELL_DATA

    print("üöó –ó–∞–ø—É—Å–∫ —Å–∫—Ä–∞–ø–µ—Ä–∞ Hot Wheels –∑ –ø—ñ–¥—Ç—Ä–∏–º–∫–æ—é SKU")
    print("=" * 60)

    response = requests.get(BASE_URL + "1", headers=HEADERS)
    soup = BeautifulSoup(response.text, 'html.parser')
    pagination = soup.find_all('li', class_='item')
    max_pages = max([int(li['data-p']) for li in pagination if 'data-p' in li.attrs], default=1)

    current_page = 0
    iteration = 1
    if os.path.exists(PROGRESS_FILE):
        with open(PROGRESS_FILE, "r") as f:
            current_page = int(f.read().strip())
    else:
        with open(PROGRESS_FILE, "w") as f:
            f.write(str(current_page))

    if current_page >= max_pages:
        current_page = 0
        with open(PROGRESS_FILE, "w") as f:
            f.write(str(current_page))

    start_page = current_page + 1
    end_page = ((start_page - 1 + PAGES_PER_DAY) % max_pages) + 1

    print(f"üìä Start page: {start_page}, End page: {end_page}, Max pages: {max_pages}")
    print("=" * 60)

    while (start_page != end_page) or (iteration < PAGES_PER_DAY):
        if not scrape_page(start_page):
            print(f"–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ {start_page}")
            break

        if start_page % SAVE_INTERVAL == 0 or start_page == end_page:
            with DATA_LOCK:
                if BUY_DATA:
                    update_csv(BUY_OUTPUT_FILE, BUY_DATA, 'price')
                    BUY_DATA = []
                if SELL_DATA:
                    update_csv(SELL_OUTPUT_FILE, SELL_DATA, 'price')
                    SELL_DATA = []

        start_page += 1
        iteration += 1
        if start_page > max_pages:
            start_page = 0
        time.sleep(2)

        with open(PROGRESS_FILE, "w") as f:
            f.write(str(start_page))

    if BUY_DATA or SELL_DATA:
        with DATA_LOCK:
            if BUY_DATA:
                update_csv(BUY_OUTPUT_FILE, BUY_DATA, 'price')
                BUY_DATA = []
            if SELL_DATA:
                update_csv(SELL_OUTPUT_FILE, SELL_DATA, 'price')
                SELL_DATA = []
    else:
        print("–ù–µ–º–∞—î –¥–∞–Ω–∏—Ö –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è")

    print("=" * 60)
    print("‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")


if __name__ == "__main__":
    main()